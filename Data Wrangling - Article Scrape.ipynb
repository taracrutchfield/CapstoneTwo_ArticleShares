{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrape\n",
    "When I was first looking at the original dataset, I found that many of the entries were missing a data channel label. So I decided I'd look into it myself and scrape the data channels, as well as the publish dates, of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# BeautifulSoup for the Soul\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Scrape was taking too long so multithreading!\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "raw = pd.read_csv(\"OnlineNewsPopularity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's so many articles, I used multithreading to cut down some of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(session,url,n):\n",
    "    '''Takes url to find the article data channel and date, if these entries cannot be found in the html, uses\n",
    "    the entry 'Unlabeled' for data channel and np.Nan for the data. Saves these values in a dictionary with \n",
    "    url as the key.'''\n",
    "    \n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # At some point Mashable changed the style of their article urls, replacing \n",
    "    # the date with the word 'article'. In the case that the origional page does\n",
    "    # not exist, makes the replacement and tries again. \n",
    "    try:\n",
    "        if str(soup.findAll('h1')[0]) == '<h1>The Bad News</h1>':\n",
    "            url = url[:20]+'article'+url[30:]\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "    except:\n",
    "        new_channels[url] = ('Unlabeled',np.NaN)\n",
    "        print('\\r%.3f%%' % (len(new_channels)/39644 * 100),end=\"\")\n",
    "        return\n",
    "    \n",
    "    # Find the labeled data channel. If it can't be found, use the entry \"Unlabeled\"\n",
    "    try:\n",
    "        channel = soup.findAll('article',{'class':'full post story'})[0]['data-channel']\n",
    "    except:\n",
    "        channel = 'Unlabeled'\n",
    "        \n",
    "    # Find the publish date. If it can't be found then replace the entry with np.NaN   \n",
    "    try:\n",
    "        #date = soup.findAll('time')[0]['datetime']\n",
    "        date = str(soup.findAll('div',{'class':'article-info'})[0].findAll('time')[0])[16:33]\n",
    "    except:\n",
    "        date = np.NaN\n",
    "        \n",
    "    # Create a dictionary entry to save the results\n",
    "    new_channels[url] = (channel,date)\n",
    "    \n",
    "    # A progress bar\n",
    "    print('\\r%.3f%%' % (len(new_channels)/n * 100),end=\"\")\n",
    "\n",
    "async def get_data_asynchronus(url_list):\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        with requests.Session() as session:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            tasks = [\n",
    "                loop.run_in_executor(\n",
    "                    executor,\n",
    "                    fetch,\n",
    "                    *(session,url,len(url_list))\n",
    "                )\n",
    "                for url in url_list\n",
    "            ]\n",
    "            for response in await asyncio.gather(*tasks):\n",
    "                pass\n",
    "\n",
    "def main(url_list):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_data_asynchronus(url_list))\n",
    "    loop.run_until_complete(future)\n",
    "    print(\"   [Process Complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.000%   [Process Complete]\n"
     ]
    }
   ],
   "source": [
    "# create my empty new_channel dictionary and begin the scrape!\n",
    "new_channels=dict()\n",
    "main(raw['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn that weird dictionary into a pandas dataframe so I can turn it into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict entries to dataframe\n",
    "updated = pd.DataFrame(new_channels).T.reset_index()\n",
    "updated.columns = ['url', 'data_channel','date']\n",
    "\n",
    "# sperate the time stamp into weekday and date\n",
    "updated['weekday'] = updated['date'].str.split(\",\", n = 1, expand = True)[0]\n",
    "updated['date'] = pd.to_datetime(updated['date'].str.split(\",\", n = 1, expand = True)[1])\n",
    "\n",
    "# to the csv!\n",
    "updated.to_csv('Updated_Data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/crayon-creatures/</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/beewi-smart-toys/</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   data_channel  \\\n",
       "0  http://mashable.com/2013/01/07/astronaut-notre...  Entertainment   \n",
       "1  http://mashable.com/2013/01/07/amazon-instant-...  Entertainment   \n",
       "2   http://mashable.com/2013/01/07/crayon-creatures/  Entertainment   \n",
       "3   http://mashable.com/2013/01/07/beewi-smart-toys/           Tech   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/           Tech   \n",
       "\n",
       "        date weekday  \n",
       "0 2013-01-07     Mon  \n",
       "1 2013-01-07     Mon  \n",
       "2 2013-01-07     Mon  \n",
       "3 2013-01-07     Mon  \n",
       "4 2013-01-07     Mon  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
