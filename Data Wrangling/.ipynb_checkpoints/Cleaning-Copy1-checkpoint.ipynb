{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "raw = pd.read_csv(\"../OnlineNewsPopularity.csv\").set_index('url')\n",
    "\n",
    "# Results of my own scraping\n",
    "update_data = pd.read_csv(\"Updates.csv\").set_index('url')\n",
    "\n",
    "print(len(update_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates Dataframe\n",
    "Since this was data scrapped from the articles, its bound to be a bit messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/astronaut-notre-dame-bcs/</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>this astronaut is rooting for notre dame tonight</td>\n",
       "      <td>space, college football, entertainment, sports</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/earth-size-planets-milky-way/</th>\n",
       "      <td>World</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>there are 17 billion earth-size alien planets ...</td>\n",
       "      <td>alien planets, earth, space, world</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/apple-40-billion-app-downloads/</th>\n",
       "      <td>Business</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>apple's app store passes 40 billion downloads</td>\n",
       "      <td>apple, apps, apps and software, business, mobile</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/downton-abbey-tumblrs/</th>\n",
       "      <td>Culture</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>8 'downton abbey' tumblrs suitable for aristoc...</td>\n",
       "      <td>downton abbey, gallery, memes, tumblr, televis...</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/att-u-verse-apps/</th>\n",
       "      <td>Tech</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>new u-verse apps simplify sharing photos and v...</td>\n",
       "      <td>apps, apps and software, at&amp;t, ces, tech</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          channel       date  \\\n",
       "url                                                                            \n",
       "http://mashable.com/2013/01/07/astronaut-notre-...  Entertainment 2013-01-07   \n",
       "http://mashable.com/2013/01/07/earth-size-plane...          World 2013-01-07   \n",
       "http://mashable.com/2013/01/07/apple-40-billion...       Business 2013-01-07   \n",
       "http://mashable.com/2013/01/07/downton-abbey-tu...        Culture 2013-01-07   \n",
       "http://mashable.com/2013/01/07/att-u-verse-apps/             Tech 2013-01-07   \n",
       "\n",
       "                                                                                                title  \\\n",
       "url                                                                                                     \n",
       "http://mashable.com/2013/01/07/astronaut-notre-...   this astronaut is rooting for notre dame tonight   \n",
       "http://mashable.com/2013/01/07/earth-size-plane...  there are 17 billion earth-size alien planets ...   \n",
       "http://mashable.com/2013/01/07/apple-40-billion...      apple's app store passes 40 billion downloads   \n",
       "http://mashable.com/2013/01/07/downton-abbey-tu...  8 'downton abbey' tumblrs suitable for aristoc...   \n",
       "http://mashable.com/2013/01/07/att-u-verse-apps/    new u-verse apps simplify sharing photos and v...   \n",
       "\n",
       "                                                                                             keywords  \\\n",
       "url                                                                                                     \n",
       "http://mashable.com/2013/01/07/astronaut-notre-...     space, college football, entertainment, sports   \n",
       "http://mashable.com/2013/01/07/earth-size-plane...                 alien planets, earth, space, world   \n",
       "http://mashable.com/2013/01/07/apple-40-billion...   apple, apps, apps and software, business, mobile   \n",
       "http://mashable.com/2013/01/07/downton-abbey-tu...  downton abbey, gallery, memes, tumblr, televis...   \n",
       "http://mashable.com/2013/01/07/att-u-verse-apps/             apps, apps and software, at&t, ces, tech   \n",
       "\n",
       "                                                   weekday  \n",
       "url                                                         \n",
       "http://mashable.com/2013/01/07/astronaut-notre-...     Mon  \n",
       "http://mashable.com/2013/01/07/earth-size-plane...     Mon  \n",
       "http://mashable.com/2013/01/07/apple-40-billion...     Mon  \n",
       "http://mashable.com/2013/01/07/downton-abbey-tu...     Mon  \n",
       "http://mashable.com/2013/01/07/att-u-verse-apps/       Mon  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update = update_data.copy()\n",
    "\n",
    "# Sperate the time stamp into weekday and date\n",
    "update['weekday'] = update['date'].str.split(\",\", n = 1, expand = True)[0]\n",
    "update['date'] = pd.to_datetime(update['date'].str.split(\",\", n = 1, expand = True)[1])\n",
    "\n",
    "# Edit text of titles and keywords\n",
    "def remove_text(column,text_list):\n",
    "    update[column] = update[column].str.lower()\n",
    "    for string in text_list:\n",
    "        update[column] = update[column].str.replace(string,'')\n",
    "        \n",
    "remove = [r\"\\<.*?\\>\",'amp;','[',']']\n",
    "remove_text('keywords',remove)\n",
    "remove_text('title',remove)\n",
    "\n",
    "update.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "split the keywords columns into something useable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/astronaut-notre-dame-bcs/</th>\n",
       "      <td>space</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/earth-size-planets-milky-way/</th>\n",
       "      <td>alien planets</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/apple-40-billion-app-downloads/</th>\n",
       "      <td>apple</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/downton-abbey-tumblrs/</th>\n",
       "      <td>downton abbey</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://mashable.com/2013/01/07/att-u-verse-apps/</th>\n",
       "      <td>apps</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          keyword  shares\n",
       "url                                                                      \n",
       "http://mashable.com/2013/01/07/astronaut-notre-...          space    1200\n",
       "http://mashable.com/2013/01/07/earth-size-plane...  alien planets    1600\n",
       "http://mashable.com/2013/01/07/apple-40-billion...          apple    1500\n",
       "http://mashable.com/2013/01/07/downton-abbey-tu...  downton abbey     761\n",
       "http://mashable.com/2013/01/07/att-u-verse-apps/             apps     505"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the keywords into seperate entries\n",
    "keywords_df = update.keywords.dropna().str.split(', ',expand=True).reset_index()\n",
    "keywords_df = pd.melt(keywords_df,id_vars=['url'],value_name='keyword')\n",
    "keywords_df = keywords_df.dropna().drop(columns='variable')\n",
    "\n",
    "# Create a new data frame made up of keywords, shares, and url\n",
    "keyword_shares = keywords_df.merge(raw.shares, how='left',on='url')\n",
    "keyword_shares = keyword_shares[keyword_shares.url != ''].set_index('url')\n",
    "keyword_shares.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop keywords only used once\n",
    "keyword_counts = keyword_shares.keyword.value_counts()\n",
    "keyword_counts = keyword_counts[keyword_counts>1].index\n",
    "\n",
    "keyword_shares = keyword_shares.reset_index().set_index('keyword').loc[keyword_counts]\n",
    "keyword_shares = keyword_shares.reset_index().set_index('url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the min, avg, and max of each keyword\n",
    "kw_min_max = keyword_shares.groupby('keyword').max().rename(columns={'shares':'max'})\n",
    "kw_min_max['avg'] = keyword_shares.groupby('keyword').mean().astype('int')\n",
    "kw_min_max['min'] = keyword_shares.groupby('keyword').min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the feautres of the best, worst, and middle keywords\n",
    "\n",
    "kw_dict = {'url':[],\n",
    "          'kw_min':[],'kw_min_min':[],'kw_min_avg':[],'kw_min_max':[],\n",
    "          'kw_avg':[],'kw_avg_min':[],'kw_avg_avg':[],'kw_avg_max':[],\n",
    "          'kw_max':[],'kw_max_min':[],'kw_max_avg':[],'kw_max_max':[]}\n",
    "\n",
    "for url in keyword_shares.index.unique():\n",
    "    article = keyword_shares.loc[url]\n",
    "    if type(article) == pd.core.series.Series:\n",
    "        key = article.loc['keyword']\n",
    "        min_share = kw_min_max.loc[key]['min']\n",
    "        avg_share = kw_min_max.loc[key]['avg']\n",
    "        max_share = kw_min_max.loc[key]['max']\n",
    "        \n",
    "        kw_dict['url'].append(url)\n",
    "        \n",
    "        for col in ['kw_min','kw_avg','kw_max']:\n",
    "            kw_dict[col].append(key)\n",
    "            \n",
    "        for col in ['min','avg','max']:\n",
    "            kw_dict['kw_'+col+'_min'].append(min_share)\n",
    "            kw_dict['kw_'+col+'_avg'].append(avg_share)\n",
    "            kw_dict['kw_'+col+'_max'].append(max_share)\n",
    "        \n",
    "    if type(article) == pd.core.frame.DataFrame:\n",
    "        keys = article['keyword'].values\n",
    "        article_kw = kw_min_max.loc[keys].sort_values('avg').reset_index()\n",
    "  \n",
    "        kw_min = article_kw.iloc[0]\n",
    "        kw_max = article_kw.iloc[-1]\n",
    "        kw_avg = article_kw.iloc[int(len(article)/2)]\n",
    "          \n",
    "        kw_dict['url'].append(url)\n",
    "    \n",
    "        kw_dict['kw_min'].append(kw_min['keyword'])\n",
    "        kw_dict['kw_min_min'].append(kw_min['min'])\n",
    "        kw_dict['kw_min_avg'].append(kw_min['avg'])\n",
    "        kw_dict['kw_min_max'].append(kw_min['max'])\n",
    "        \n",
    "        kw_dict['kw_avg'].append(kw_avg['keyword'])\n",
    "        kw_dict['kw_avg_min'].append(kw_avg['min'])\n",
    "        kw_dict['kw_avg_avg'].append(kw_avg['avg'])\n",
    "        kw_dict['kw_avg_max'].append(kw_avg['max'])\n",
    "        \n",
    "        kw_dict['kw_max'].append(kw_max['keyword'])\n",
    "        kw_dict['kw_max_min'].append(kw_max['min'])\n",
    "        kw_dict['kw_max_avg'].append(kw_max['avg'])\n",
    "        kw_dict['kw_max_max'].append(kw_max['max'])\n",
    "\n",
    "kw_min_max_all = pd.DataFrame(kw_dict).set_index('url')\n",
    "kw_min_max_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the origional with out new features\n",
    "temp_data = raw.drop(columns=['kw_min_min','kw_min_avg','kw_min_max',\n",
    "                          'kw_avg_min','kw_avg_avg','kw_avg_max',\n",
    "                          'kw_max_min','kw_max_avg','kw_max_max'])\n",
    "update = update.merge(kw_min_max_all,how='right',right_index=True,left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = temp_data.reset_index().join(update, on='url',how='inner').set_index('url').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days of the Week\n",
    "First of, since we have a new days of the week column, I'm going to drop the origionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['weekday_is_monday','weekday_is_tuesday','weekday_is_wednesday',\n",
    "                          'weekday_is_thursday','weekday_is_friday','weekday_is_saturday',\n",
    "                          'weekday_is_sunday','is_weekend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_down(column,sort='total articles',show=True):\n",
    "    counts = data[[column,'shares']].groupby(by=column).count().rename(columns={'shares':'total articles'})\n",
    "    sums = data[[column,'shares']].groupby(by=column).sum().rename(columns={'shares':'sum shares'})\n",
    "    group = counts.join(sums,on=column)\n",
    "    \n",
    "    # add the shares per article ratio\n",
    "    group[\"shares per article ratio\"] = group[\"sum shares\"]/group[\"total articles\"]\n",
    "\n",
    "    # add percentages to make it easier to see\n",
    "    group['percent of all shares'] = group['sum shares']/group['sum shares'].sum()*100\n",
    "    group['percent of articles'] = group['total articles']/group['total articles'].sum()*100\n",
    "\n",
    "    # sort\n",
    "    group = group.sort_values('total articles',ascending=False)\n",
    "    \n",
    "    if show==True:\n",
    "        group\n",
    "    \n",
    "    return group\n",
    "\n",
    "break_down('weekday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like less articles are published on Saturday and Sunday so I combined both these entries as `weekend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'] = data['weekday'].replace(['Sat','Sun'],'Weekend')\n",
    "break_down('weekday')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Channels\n",
    "Do the same with data channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['data_channel_is_lifestyle',' data_channel_is_entertainment','data_channel_is_bus',\n",
    "                          'data_channel_is_socmed','data_channel_is_tech','data_channel_is_world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_down('channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most articles are from the `Entertainment` category, followed by `World` and `Tech`. Understandably the highest sum shares come from the same categories.\n",
    "\n",
    "Since `Social Good`, `US`, and the unlabeled data channels are so small, I just grouped them up into one category labeled `Other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['channel'] = data['channel'].replace(['Social Good','U.S.','Unlabeled'],'Other')\n",
    "break_down('channel') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Shares\n",
    "Now that the weekdays and channels have been updated, lets look at the shares as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5),dpi=200)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(data['shares'],bins=20);\n",
    "plt.title('Distribution of Shares')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(data['shares'],bins=20,range=(0,10000))\n",
    "plt.title('Distribution of Shares - Zoomed In');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there's alot of outlires. Viral articles get a whole lot more shares than the typical article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved the data as `OnlineNewsPopularity_Cleaned.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../OnlineNewsPopularity_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
